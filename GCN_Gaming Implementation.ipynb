{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the network\n",
    "import networkx as nx\n",
    "\n",
    "def create_Gaming_network():\n",
    "    G = nx.Graph()\n",
    "    filename1 = 'C:/Vidit/PhD/RA Work/KZ - RA/Gaming Project/data/Files/game_user.txt'\n",
    "    filename2 = 'C:/Vidit/PhD/RA Work/KZ - RA/Gaming Project/data/Files/item_game.txt'\n",
    "    filename3 = 'C:/Vidit/PhD/RA Work/KZ - RA/Gaming Project/data/Files/item_user.txt'\n",
    "    filename4 = 'C:/Vidit/PhD/RA Work/KZ - RA/Gaming Project/data/Files/user_friend.txt'\n",
    "\n",
    "    # Adding User-Game connections\n",
    "    user_game = []\n",
    "    with open(filename1) as f1:\n",
    "        for line in f1:\n",
    "            tup = ()\n",
    "            l = []\n",
    "            toks = line.strip().split(\"\\t\")\n",
    "            user = 'u' + toks[1]\n",
    "            game = 'g' + toks[0]\n",
    "            l.append(user)\n",
    "            l.append(game)\n",
    "            tup = tup + tuple(l)\n",
    "            user_game.append(tup)\n",
    "    f1.close()\n",
    "    G.add_edges_from(user_game)  # Adding to the network\n",
    "\n",
    "    # Adding Game-Item connections\n",
    "    game_item = []\n",
    "    with open(filename2) as f2:\n",
    "        for line in f2:\n",
    "            tup = ()\n",
    "            l = []\n",
    "            toks = line.strip().split(\"\\t\")\n",
    "            game = 'g' + toks[1]\n",
    "            item = 'i' + toks[0]\n",
    "            l.append(game)\n",
    "            l.append(item)\n",
    "            tup = tup + tuple(l)\n",
    "            game_item.append(tup)\n",
    "    f2.close()\n",
    "    G.add_edges_from(game_item)  # Adding to the network\n",
    "\n",
    "    # Adding User-Item connections\n",
    "    user_item = []\n",
    "    with open(filename3) as f3:\n",
    "        for line in f3:\n",
    "            tup = ()\n",
    "            l = []\n",
    "            toks = line.strip().split(\"\\t\")\n",
    "            user = 'u' + toks[1]\n",
    "            item = 'i' + toks[0]\n",
    "            l.append(user)\n",
    "            l.append(item)\n",
    "            tup = tup + tuple(l)\n",
    "            user_item.append(tup)\n",
    "    f3.close()\n",
    "    G.add_edges_from(user_item)  # Adding to the network\n",
    "\n",
    "    # Adding User-Friend connections\n",
    "    user_friend = []\n",
    "    with open(filename4) as f4:\n",
    "        for line in f4:\n",
    "            tup = ()\n",
    "            l = []\n",
    "            toks = line.strip().split(\"\\t\")\n",
    "            user = 'u' + toks[0]\n",
    "            friend = 'u' + toks[1]\n",
    "            l.append(user)\n",
    "            l.append(friend)\n",
    "            tup = tup + tuple(l)\n",
    "            user_friend.append(tup)\n",
    "    f4.close()\n",
    "    #G.add_edges_from(user_friend)  # Adding to the network\n",
    "    #print(\"Numer of nodes in network:\",G.number_of_nodes())\n",
    "    #print(\"Numer of edges in network:\",G.number_of_edges())\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup of Matrices\n",
    "\n",
    "from networkx import to_numpy_matrix, to_scipy_sparse_matrix\n",
    "import scipy.sparse as sp\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from scipy.linalg import fractional_matrix_power\n",
    "import torch\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    G = create_Gaming_network()\n",
    "    order = sorted(list(G.nodes()))\n",
    "    A = to_scipy_sparse_matrix(G, nodelist=order)\n",
    "    I = sp.eye(A.shape[0])\n",
    "    A_hat = A + I  # Adding Self-loops\n",
    "    #D = np.array(np.sum(A_hat, axis=0))[0]  # getting degree of all nodes\n",
    "    #D = np.matrix(np.diag(D))   # degree matrix is a diagonal matrix\n",
    "    D = np.array(A_hat.sum(1))  # getting degree of all nodes\n",
    "    D_inv = np.power(D, -0.5).flatten()\n",
    "    D_inv[np.isinf(D_inv)] = 0\n",
    "    D_inv = sp.diags(D_inv)\n",
    "    A = sparse_mx_to_torch_sparse_tensor(A)\n",
    "    I = sparse_mx_to_torch_sparse_tensor(I)\n",
    "    A_hat = sparse_mx_to_torch_sparse_tensor(A_hat)\n",
    "    D_inv = sparse_mx_to_torch_sparse_tensor(D_inv)\n",
    "    A_old = torch.mul(A_hat, D_inv)\n",
    "    A_new = torch.mul(D_inv, A_old)\n",
    "    return I, G, A_new\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float64)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GCN Layers and GCN Model Step\n",
    "\n",
    "import math\n",
    "import torch\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "class GraphConvolution(Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.DoubleTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.DoubleTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.mm(adj, support)\n",
    "        return output\n",
    "\n",
    "    \n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_features, out_features, hidden_layer_one_units, hidden_layer_two_units, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(in_features, hidden_layer_one_units)\n",
    "        self.gc2 = GraphConvolution(hidden_layer_one_units, hidden_layer_two_units)\n",
    "        self.gc3 = GraphConvolution(hidden_layer_two_units, out_features)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):    \n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.relu(self.gc2(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.gc3(x, adj)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting P Distribution...\n"
     ]
    }
   ],
   "source": [
    "# Loss -> Optimizer -> Train\n",
    "\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import scipy.sparse\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def get_P_distribution(G):\n",
    "    print(\"Getting P Distribution...\")\n",
    "    distances = []\n",
    "    for i in sorted(list(G.nodes)):\n",
    "        for j in sorted(list(G.nodes)):\n",
    "            if i != j:\n",
    "\n",
    "                temp_distance = nx.shortest_path_length(G, i, j)\n",
    "                distances.append(temp_distance)\n",
    "    \n",
    "    mx = scipy.sparse.csr_matrix(distances)\n",
    "    mx = sparse_mx_to_torch_sparse_tensor(mx)\n",
    "    sum = torch.sparse.sum(mx)\n",
    "    distances_norm = torch.div(mx,sum)\n",
    "    distances_norm = Variable(distances_norm, requires_grad = True)\n",
    "    return distances_norm\n",
    "\n",
    "def get_Q_distribution(output):\n",
    "    print(\"Getting Q Distribution...\")\n",
    "    distances = []\n",
    "    for row_num1, node1 in enumerate(output):\n",
    "        for row_num2, node2 in enumerate(output):\n",
    "            if row_num1 != row_num2:\n",
    "                temp_distance = torch.dist(node1, node2)\n",
    "                distances.append(temp_distance.tolist())\n",
    "    \n",
    "    for i in range(0,len(distances)):\n",
    "        if distances[i]==0:\n",
    "            distances[i] = 0.000001\n",
    "    mx = scipy.sparse.csr_matrix(distances)\n",
    "    mx = sparse_mx_to_torch_sparse_tensor(mx)\n",
    "    sum = torch.sparse.sum(mx)\n",
    "    distances_norm = torch.div(mx,sum)\n",
    "    return distances_norm\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float64)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "\n",
    "# Load data\n",
    "I, G, A_new = load_data()\n",
    "P = get_P_distribution(G)\n",
    "\n",
    "# Model, Loss and optimizer\n",
    "model = GCN(in_features = I.shape[1], out_features = 128, hidden_layer_one_units = 64, hidden_layer_two_units = 32, dropout = 0.5)\n",
    "loss_fn = nn.KLDivLoss(reduction='sum')\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.01)\n",
    "\n",
    "\n",
    "def get_old_Q_distribution(output):\n",
    "    m = []\n",
    "    for node1 in output:\n",
    "        l = []\n",
    "        for node2 in output:\n",
    "            d = torch.dist(node1,node2)\n",
    "            l.append(d.tolist())\n",
    "        m.append(l)\n",
    "    y = np.asarray(m)\n",
    "    return(torch.from_numpy(np.matrix(y/y.sum(axis=1, keepdims = True))))\n",
    "    \n",
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    # Forward Propagation\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model.forward(I, A_new)\n",
    "    Q = get_Q_distribution(output)\n",
    "    loss = loss_fn(P.coalesce().values().log(),Q.coalesce().values())\n",
    "    \n",
    "    # Back Propagation\n",
    "    loss = Variable(loss, requires_grad = True)\n",
    "    print(\"loss=\", loss)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return output\n",
    "\n",
    "# Train model\n",
    "t_total = time.time()\n",
    "for epoch in range(2):\n",
    "    train(epoch)\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "# Train model\n",
    "t_total = time.time()\n",
    "I, G, A_new = load_data()\n",
    "for epoch in range(2):\n",
    "    output = train(epoch)\n",
    "nodes = sorted(list(G.nodes))\n",
    "output_embedding = output.tolist()\n",
    "filename = \"C:/Vidit/PhD/RA Work/KZ - RA/Gaming Project/data/GCN/GCN_Gaming_Embeddings.txt\"\n",
    "result = list(zip(nodes,output_embedding))\n",
    "with open(filename,'w') as f:\n",
    "    for x in result:\n",
    "        f.write(str(x[0]) + \" \")\n",
    "        for y in x[1]:\n",
    "            f.write(str(y) + \" \")\n",
    "        f.write(\"\\n\")\n",
    "f.close()\n",
    "print(\"Optimization Finished successfully!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the network\n",
    "import networkx as nx\n",
    "\n",
    "def create_Gaming_network():\n",
    "    G = nx.Graph()\n",
    "    filename1 = 'C:/Vidit/PhD/RA Work/KZ - RA/Gaming Project/data/Files/game_user.txt'\n",
    "    filename2 = 'C:/Vidit/PhD/RA Work/KZ - RA/Gaming Project/data/Files/item_game.txt'\n",
    "    filename3 = 'C:/Vidit/PhD/RA Work/KZ - RA/Gaming Project/data/Files/item_user.txt'\n",
    "    filename4 = 'C:/Vidit/PhD/RA Work/KZ - RA/Gaming Project/data/Files/user_friend.txt'\n",
    "\n",
    "    # Adding User-Game connections\n",
    "    user_game = []\n",
    "    with open(filename1) as f1:\n",
    "        for line in f1:\n",
    "            tup = ()\n",
    "            l = []\n",
    "            toks = line.strip().split(\"\\t\")\n",
    "            user = 'u' + toks[1]\n",
    "            game = 'g' + toks[0]\n",
    "            l.append(user)\n",
    "            l.append(game)\n",
    "            tup = tup + tuple(l)\n",
    "            user_game.append(tup)\n",
    "    f1.close()\n",
    "    G.add_edges_from(user_game)  # Adding to the network\n",
    "\n",
    "    # Adding Game-Item connections\n",
    "    game_item = []\n",
    "    with open(filename2) as f2:\n",
    "        for line in f2:\n",
    "            tup = ()\n",
    "            l = []\n",
    "            toks = line.strip().split(\"\\t\")\n",
    "            game = 'g' + toks[1]\n",
    "            item = 'i' + toks[0]\n",
    "            l.append(game)\n",
    "            l.append(item)\n",
    "            tup = tup + tuple(l)\n",
    "            game_item.append(tup)\n",
    "    f2.close()\n",
    "    G.add_edges_from(game_item)  # Adding to the network\n",
    "\n",
    "    # Adding User-Item connections\n",
    "    user_item = []\n",
    "    with open(filename3) as f3:\n",
    "        for line in f3:\n",
    "            tup = ()\n",
    "            l = []\n",
    "            toks = line.strip().split(\"\\t\")\n",
    "            user = 'u' + toks[1]\n",
    "            item = 'i' + toks[0]\n",
    "            l.append(user)\n",
    "            l.append(item)\n",
    "            tup = tup + tuple(l)\n",
    "            user_item.append(tup)\n",
    "    f3.close()\n",
    "    G.add_edges_from(user_item)  # Adding to the network\n",
    "\n",
    "    # Adding User-Friend connections\n",
    "    user_friend = []\n",
    "    with open(filename4) as f4:\n",
    "        for line in f4:\n",
    "            tup = ()\n",
    "            l = []\n",
    "            toks = line.strip().split(\"\\t\")\n",
    "            user = 'u' + toks[0]\n",
    "            friend = 'u' + toks[1]\n",
    "            l.append(user)\n",
    "            l.append(friend)\n",
    "            tup = tup + tuple(l)\n",
    "            user_friend.append(tup)\n",
    "    f4.close()\n",
    "    #G.add_edges_from(user_friend)  # Adding to the network\n",
    "    #print(\"Numer of nodes in network:\",G.number_of_nodes())\n",
    "    #print(\"Numer of edges in network:\",G.number_of_edges())\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup of Matrices\n",
    "\n",
    "from networkx import to_numpy_matrix, to_scipy_sparse_matrix\n",
    "import scipy.sparse as sp\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from scipy.linalg import fractional_matrix_power\n",
    "import torch\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    G = create_Gaming_network()\n",
    "    order = sorted(list(G.nodes()))\n",
    "    A = to_scipy_sparse_matrix(G, nodelist=order)\n",
    "    I = sp.eye(A.shape[0])\n",
    "    A_hat = A + I  # Adding Self-loops\n",
    "    #D = np.array(np.sum(A_hat, axis=0))[0]  # getting degree of all nodes\n",
    "    #D = np.matrix(np.diag(D))   # degree matrix is a diagonal matrix\n",
    "    D = np.array(A_hat.sum(1))  # getting degree of all nodes\n",
    "    D_inv = np.power(D, -0.5).flatten()\n",
    "    D_inv[np.isinf(D_inv)] = 0\n",
    "    D_inv = sp.diags(D_inv)\n",
    "    mx = D_inv.dot(A_hat)\n",
    "    new_mx = mx.dot(D_inv)\n",
    "    A = sparse_mx_to_torch_sparse_tensor(A)\n",
    "    I = sparse_mx_to_torch_sparse_tensor(I)\n",
    "    A_hat = sparse_mx_to_torch_sparse_tensor(A_hat)\n",
    "    D_inv = sparse_mx_to_torch_sparse_tensor(D_inv)\n",
    "    A_new = sparse_mx_to_torch_sparse_tensor(new_mx)\n",
    "    #A_old = torch.mul(A_hat, D_inv)\n",
    "    #A_new = torch.mul(D_inv, A_old)\n",
    "    return I, G, A_new\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float64)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GCN Layers and GCN Model Step\n",
    "\n",
    "import math\n",
    "import torch\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_features, out_features, hidden_layer_one_units, hidden_layer_two_units):\n",
    "        super(GCN, self).__init__()\n",
    "        \n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.hidden_layer_one_units = hidden_layer_one_units\n",
    "        self.hidden_layer_two_units = hidden_layer_two_units\n",
    "        #self.dropout = dropout\n",
    "        self.weight_one = Parameter(torch.rand(in_features, hidden_layer_one_units, requires_grad=True))\n",
    "        self.weight_two = Parameter(torch.rand(hidden_layer_one_units, hidden_layer_two_units, requires_grad=True))\n",
    "        self.weight_three = Parameter(torch.rand(hidden_layer_two_units, out_features, requires_grad=True))\n",
    "        #if bias:\n",
    "        #    self.bias = Parameter(torch.rand(out_features))\n",
    "        #else:\n",
    "        #    self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        stdv1 = 1. / math.sqrt(self.weight_one.size(1))\n",
    "        self.weight_one.data.uniform_(-stdv1, stdv1)\n",
    "        \n",
    "        stdv2 = 1. / math.sqrt(self.weight_two.size(1))\n",
    "        self.weight_two.data.uniform_(-stdv2, stdv2)\n",
    "        \n",
    "        stdv3 = 1. / math.sqrt(self.weight_three.size(1))\n",
    "        self.weight_three.data.uniform_(-stdv3, stdv3)\n",
    "\n",
    "    def gcn_layer_output(self, input, adj, weight):\n",
    "        support = torch.mm(input, weight.double())\n",
    "        output = torch.mm(adj, support)\n",
    "        return output\n",
    "\n",
    "    def forward(self, x, adj):    \n",
    "        x = F.relu(self.gcn_layer_output(x, adj, self.weight_one))\n",
    "        #x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.relu(self.gcn_layer_output(x, adj, self.weight_two))\n",
    "        #x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.relu(self.gcn_layer_output(x, adj, self.weight_three))\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Finished successfully!\n",
      "Total time elapsed: 658.2307s\n"
     ]
    }
   ],
   "source": [
    "# Loss -> Optimizer -> Train\n",
    "\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import scipy.sparse\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def get_P_distribution(G):\n",
    "    distances = []\n",
    "    for i in sorted(list(G.nodes)):\n",
    "        for j in sorted(list(G.nodes)):\n",
    "            if i != j:\n",
    "\n",
    "                temp_distance = nx.shortest_path_length(G, i, j)\n",
    "                distances.append(temp_distance)\n",
    "    \n",
    "    mx = scipy.sparse.csr_matrix(distances)\n",
    "    mx = sparse_mx_to_torch_sparse_tensor(mx)\n",
    "    sum = torch.sparse.sum(mx)\n",
    "    distances_norm = torch.div(mx,sum)\n",
    "    distances_norm = Variable(distances_norm, requires_grad = True)\n",
    "    return distances_norm\n",
    "\n",
    "def get_Q_distribution(output):\n",
    "    distances = []\n",
    "    for row_num1, node1 in enumerate(output):\n",
    "        for row_num2, node2 in enumerate(output):\n",
    "            if row_num1 != row_num2:\n",
    "                temp_distance = torch.dist(node1, node2)\n",
    "                distances.append(temp_distance.tolist())\n",
    "    \n",
    "    for i in range(0,len(distances)):\n",
    "        if distances[i]==0:\n",
    "            distances[i] = 0.000001\n",
    "    mx = scipy.sparse.csr_matrix(distances)\n",
    "    mx = sparse_mx_to_torch_sparse_tensor(mx)\n",
    "    sum = torch.sparse.sum(mx)\n",
    "    distances_norm = torch.div(mx,sum)\n",
    "    return distances_norm\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float64)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "\n",
    "# Load data\n",
    "I, G, A_new = load_data()\n",
    "\n",
    "# Model, Loss and optimizer\n",
    "model = GCN(in_features = I.shape[1], out_features = 128, hidden_layer_one_units = 128, hidden_layer_two_units = 128)\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.01)\n",
    "\n",
    "\n",
    "def get_old_Q_distribution(output):\n",
    "    m = []\n",
    "    for node1 in output:\n",
    "        l = []\n",
    "        for node2 in output:\n",
    "            d = torch.dist(node1,node2)\n",
    "            l.append(d.tolist())\n",
    "        m.append(l)\n",
    "    y = np.asarray(m)\n",
    "    return(torch.from_numpy(np.matrix(y/y.sum(axis=1, keepdims = True))))\n",
    "    \n",
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    # Forward Propagation\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model.forward(I, A_new)\n",
    "    optimizer.step()\n",
    "    return output\n",
    "\n",
    "# Train model\n",
    "t_total = time.time()\n",
    "I, G, A_new = load_data()\n",
    "for epoch in range(10):\n",
    "    output = train(epoch)\n",
    "nodes = sorted(list(G.nodes()))\n",
    "output_embedding = output.tolist()\n",
    "filename = \"C:/Vidit/PhD/RA Work/KZ - RA/Gaming Project/data/GCN/GCN_Gaming_Base_Network_Embeddings.txt\"\n",
    "result = list(zip(nodes,output_embedding))\n",
    "with open(filename,'w') as f:\n",
    "    for x in result:\n",
    "        f.write(str(x[0]) + \" \")\n",
    "        for y in x[1]:\n",
    "            f.write(str(y) + \" \")\n",
    "        f.write(\"\\n\")\n",
    "f.close()\n",
    "print(\"Optimization Finished successfully!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#users 1496096\n",
      "#games 22\n",
      "#items 3825\n",
      "No of users who bought items in first 2 days of test period: 2700\n",
      "No of users who bought items in first 4 days of test period: 4999\n",
      "No of users who bought items in first 10 days of test period: 10302\n",
      "\n",
      "Making Recommendations\n",
      "\n",
      "\n",
      "Accuracy for Star Items\n",
      "\n",
      "\n",
      "Making Recommendations\n",
      "#Number of users randomly selected: 10000\n",
      "\n",
      "\n",
      "Accuracy for GCN Recommender System based on Top-5 similar users\n",
      "For a 2-day testing period: 0.0019\n",
      "For a 4-day testing period: 0.0048\n",
      "For a 10-day testing period: 0.0131\n",
      "\n",
      "\n",
      "Accuracy for GCN Recommender System based on Top-10 similar users\n",
      "For a 2-day testing period: 0.0036\n",
      "For a 4-day testing period: 0.0095\n",
      "For a 10-day testing period: 0.0221\n",
      "\n",
      "\n",
      "Accuracy for GCN Recommender System based on Top-15 similar users\n",
      "For a 2-day testing period: 0.0036\n",
      "For a 4-day testing period: 0.0095\n",
      "For a 10-day testing period: 0.0221\n",
      "\n",
      "\n",
      "Accuracy for GCN Recommender System based on Top-20 similar users\n",
      "For a 2-day testing period: 0.0036\n",
      "For a 4-day testing period: 0.0095\n",
      "For a 10-day testing period: 0.0221\n",
      "\n",
      "\n",
      "\n",
      "Accuracy for Long Tail Items\n",
      "\n",
      "\n",
      "Making Recommendations\n",
      "#Number of users randomly selected: 10000\n",
      "\n",
      "\n",
      "Accuracy for GCN Recommender System based on Top-5 similar users\n",
      "For a 2-day testing period: 0.0\n",
      "For a 4-day testing period: 0.0\n",
      "For a 10-day testing period: 0.0\n",
      "\n",
      "\n",
      "Accuracy for GCN Recommender System based on Top-10 similar users\n",
      "For a 2-day testing period: 0.0\n",
      "For a 4-day testing period: 0.0002\n",
      "For a 10-day testing period: 0.0005\n",
      "\n",
      "\n",
      "Accuracy for GCN Recommender System based on Top-15 similar users\n",
      "For a 2-day testing period: 0.0\n",
      "For a 4-day testing period: 0.0002\n",
      "For a 10-day testing period: 0.0006\n",
      "\n",
      "\n",
      "Accuracy for GCN Recommender System based on Top-20 similar users\n",
      "For a 2-day testing period: 0.0001\n",
      "For a 4-day testing period: 0.0003\n",
      "For a 10-day testing period: 0.0007\n",
      "\n",
      "\n",
      "\n",
      "Accuracy for All Items\n",
      "\n",
      "\n",
      "Making Recommendations\n",
      "#Number of users randomly selected: 10000\n",
      "\n",
      "\n",
      "Accuracy for GCN Recommender System based on Top-5 similar users\n",
      "For a 2-day testing period: 0.0019\n",
      "For a 4-day testing period: 0.0048\n",
      "For a 10-day testing period: 0.0131\n",
      "\n",
      "\n",
      "Accuracy for GCN Recommender System based on Top-10 similar users\n",
      "For a 2-day testing period: 0.0036\n",
      "For a 4-day testing period: 0.0096\n",
      "For a 10-day testing period: 0.0222\n",
      "\n",
      "\n",
      "Accuracy for GCN Recommender System based on Top-15 similar users\n",
      "For a 2-day testing period: 0.0036\n",
      "For a 4-day testing period: 0.0096\n",
      "For a 10-day testing period: 0.0222\n",
      "\n",
      "\n",
      "Accuracy for GCN Recommender System based on Top-20 similar users\n",
      "For a 2-day testing period: 0.0036\n",
      "For a 4-day testing period: 0.0096\n",
      "For a 10-day testing period: 0.0222\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "class RecommendVirtualItems:\n",
    "    def __init__(self):\n",
    "        self.id_user = dict()\n",
    "        self.user_id = dict()\n",
    "        self.id_game = dict()\n",
    "        self.game_id = dict()\n",
    "        self.id_item = dict()\n",
    "        self.item_id = dict()\n",
    "        self.item_user = dict()\n",
    "        self.user_item = dict()\n",
    "        self.game_user = dict()\n",
    "        self.user_game = dict()\n",
    "        self.game_item = dict()\n",
    "        self.item_game = dict()\n",
    "        self.game_userlist = dict()\n",
    "        self.user_gamelist = dict()\n",
    "        self.game_itemlist = dict()\n",
    "        self.item_gamelist = dict()\n",
    "        self.user_itemlist = dict()\n",
    "        self.item_userlist = dict()\n",
    "        self.test2days_item_user = dict()\n",
    "        self.test2days_user_item = dict()\n",
    "        self.test4days_item_user = dict()\n",
    "        self.test4days_user_item = dict()\n",
    "        self.test10days_item_user = dict()\n",
    "        self.test10days_user_item = dict()\n",
    "        self.userlist = []\n",
    "        self.star_items = []\n",
    "        self.tail_items = []\n",
    "        self.all_items = []\n",
    "\n",
    "    def read_data(self, dirpath):\n",
    "        with open(dirpath + \"/id_user.txt\") as udictfile:\n",
    "            for line in udictfile:\n",
    "                toks = line.strip().split(\"\\t\")\n",
    "                if len(toks) == 2:\n",
    "                    self.id_user[toks[0]] = toks[1].replace(\" \", \"\")    # {uid1: User1, uid2: User2....}\n",
    "                    self.user_id[toks[1]] = toks[0].replace(\" \", \"\")    # {User1: uid1, User2: uid2....}\n",
    "\n",
    "        print(\"#users\", len(self.id_user))\n",
    "\n",
    "        with open(dirpath + \"/id_game.txt\") as gdictfile:\n",
    "            for line in gdictfile:\n",
    "                toks = line.strip().split(\"\\t\")\n",
    "                if len(toks) == 2:\n",
    "                    self.id_game[toks[0]] = toks[1].replace(\" \", \"\")    # {33302: LoLsg, 33305: LoLmy....}\n",
    "                    self.game_id[toks[1]] = toks[0].replace(\" \", \"\")    # {LoLsg: 33302, LoLmy: 33305....}\n",
    "\n",
    "        print(\"#games\", len(self.id_game))\n",
    "        \n",
    "        with open(dirpath + \"/id_item.txt\") as idictfile:\n",
    "            for line in idictfile:\n",
    "                toks = line.strip().split(\"\\t\")\n",
    "                if len(toks) == 2:\n",
    "                    self.id_item[toks[0]] = toks[1].replace(\" \", \"\")    # {itemid1: Item1, itemid2: Item2....}\n",
    "                    self.item_id[toks[1]] = toks[0].replace(\" \", \"\")    # {Item1: itemid1, Item2: itemid2....}\n",
    "\n",
    "        print(\"#items\", len(self.id_item)) # comprehensive list of items\n",
    "\n",
    "        with open(dirpath + \"/game_user.txt\") as gufile:  # data pulled from user game activity for 20 days\n",
    "            for line in gufile:\n",
    "                toks = line.strip().split(\"\\t\")\n",
    "                if len(toks) == 2:\n",
    "                    g, u = toks[0], toks[1]\n",
    "                    if g not in self.game_user:\n",
    "                        self.game_user[g] = []\n",
    "                    self.game_user[g].append(u)   # {gameid1: [uid1,uid2,...], gameid2: [uid2,uid3,....]} = dict of list of users playing games\n",
    "                    if u not in self.user_game:\n",
    "                        self.user_game[u] = []\n",
    "                    self.user_game[u].append(g)   # {uid1: [gameid1,gameid2,...], uid2: [gameid2,gameid3,....]} = dict of list of games played by each user\n",
    "        \n",
    "        \n",
    "        with open(dirpath + \"/item_user.txt\") as iufile:  # list of all items purchased by users\n",
    "            for line in iufile:\n",
    "                toks = line.strip().split(\"\\t\")\n",
    "                if len(toks) == 2:\n",
    "                    i, u = toks[0], toks[1]\n",
    "                    if i not in self.item_user:\n",
    "                        self.item_user[i] = []\n",
    "                    self.item_user[i].append(u)   # {itemid1: [uid1,uid2,...], itemid2: [uid2,uid3,....]} = dict of list of users purchasing each item \n",
    "                    if u not in self.user_item:\n",
    "                        self.user_item[u] = []\n",
    "                    self.user_item[u].append(i)   # {uid1: [itemid1,itemid2,...], uid2: [itemd2,itemd3,....]} = dict of list of items purchased by each user\n",
    "        \n",
    "        \n",
    "        with open(dirpath + \"/item_game.txt\") as igfile:  # list of all in-game items\n",
    "            for line in igfile:\n",
    "                toks = line.strip().split(\"\\t\")\n",
    "                if len(toks) == 2:\n",
    "                    i, g = toks[0], toks[1]\n",
    "                    if i not in self.item_game:\n",
    "                        self.item_game[i] = []  \n",
    "                    self.item_game[i].append(g) # {itemid1: [gameid1,gameid2,...], itemid2: [gameid2,gameid3,....]} = list of items that got recommended. Every item can be in multiple games\n",
    "                    if g not in self.game_item:\n",
    "                        self.game_item[g] = []\n",
    "                    self.game_item[g].append(i) # {gameid1: [itemid1,itemid2,...], gameid2: [itemd2,itemd3,....]} = list of items available in-game for purchase\n",
    "        \n",
    "        with open(dirpath + \"/test2days_item_user.txt\") as iufile:  # list of all items purchased by users in 2 days of testing period\n",
    "            for line in iufile:\n",
    "                toks = line.strip().split(\"\\t\")\n",
    "                if len(toks) == 2:\n",
    "                    i, u = toks[0], toks[1]\n",
    "                    if i not in self.test2days_item_user:\n",
    "                        self.test2days_item_user[i] = []\n",
    "                    self.test2days_item_user[i].append(u)   # {itemid1: [uid1,uid2,...], itemid2: [uid2,uid3,....]} = dict of list of users purchasing each item \n",
    "                    if u not in self.test2days_user_item:\n",
    "                        self.test2days_user_item[u] = []\n",
    "                    self.test2days_user_item[u].append(i)   # {uid1: [itemid1,itemid2,...], uid2: [itemd2,itemd3,....]} = dict of list of items purchased by each user\n",
    "        print(\"No of users who bought items in first 2 days of test period:\", len(self.test2days_user_item))\n",
    "        \n",
    "        with open(dirpath + \"/test4days_item_user.txt\") as iufile:  # list of all items purchased by users in 4 days of testing period\n",
    "            for line in iufile:\n",
    "                toks = line.strip().split(\"\\t\")\n",
    "                if len(toks) == 2:\n",
    "                    i, u = toks[0], toks[1]\n",
    "                    if i not in self.test4days_item_user:\n",
    "                        self.test4days_item_user[i] = []\n",
    "                    self.test4days_item_user[i].append(u)   # {itemid1: [uid1,uid2,...], itemid2: [uid2,uid3,....]} = dict of list of users purchasing each item \n",
    "                    if u not in self.test4days_user_item:\n",
    "                        self.test4days_user_item[u] = []\n",
    "                    self.test4days_user_item[u].append(i)   # {uid1: [itemid1,itemid2,...], uid2: [itemd2,itemd3,....]} = dict of list of items purchased by each user\n",
    "        print(\"No of users who bought items in first 4 days of test period:\", len(self.test4days_user_item))\n",
    "        \n",
    "        with open(dirpath + \"/test10days_item_user.txt\") as iufile:  # list of all items purchased by users in 10 days of testing period\n",
    "            for line in iufile:\n",
    "                toks = line.strip().split(\"\\t\")\n",
    "                if len(toks) == 2:\n",
    "                    i, u = toks[0], toks[1]\n",
    "                    if i not in self.test10days_item_user:\n",
    "                        self.test10days_item_user[i] = []\n",
    "                    self.test10days_item_user[i].append(u)   # {itemid1: [uid1,uid2,...], itemid2: [uid2,uid3,....]} = dict of list of users purchasing each item \n",
    "                    if u not in self.test10days_user_item:\n",
    "                        self.test10days_user_item[u] = []\n",
    "                    self.test10days_user_item[u].append(i)   # {uid1: [itemid1,itemid2,...], uid2: [itemd2,itemd3,....]} = dict of list of items purchased by each user\n",
    "        print(\"No of users who bought items in first 10 days of test period:\", len(self.test10days_user_item))\n",
    "        \n",
    "        with open(dirpath + \"/selected_user.txt\") as sufile:  # pull the randomly selected users\n",
    "            for user in sufile:\n",
    "                self.userlist.append(user.strip())\n",
    "                            \n",
    "    def read_gcn_output(self, outputfilename):\n",
    "        self.user_vec = dict()\n",
    "        self.game_vec = dict()\n",
    "        self.item_vec = dict()\n",
    "        with open(outputfilename) as nvfile:             \n",
    "            for line in nvfile:\n",
    "                toks = line.strip().split(\" \")\n",
    "                if len(toks) == 129:\n",
    "                    node = toks[0]\n",
    "                    del toks[0]\n",
    "                    if node[0] == 'u': \n",
    "                        self.user_vec[node[1:]] = toks    # {uid1: [user_vector_values], uid2: [user_vector_values]..}\n",
    "                    elif node[0] == 'g':\n",
    "                        self.game_vec[node[1:]] = toks    # {gameid1: [game_vector_values], gameid2: [game_vector_values]..}\n",
    "                    elif node[0] == 'i':\n",
    "                        self.item_vec[node[1:]] = toks    # {itemid1: [item_vector_values], itemid2: [item_vector_values]..}\n",
    "        \n",
    "    def get_cosine_similarity(self, vec1, vec2):\n",
    "        a = np.array(vec1, dtype=float)\n",
    "        b = np.array(vec2, dtype=float)\n",
    "        dot_product = np.dot(a, b)\n",
    "        norm_a = np.linalg.norm(a)\n",
    "        norm_b = np.linalg.norm(b)\n",
    "        return dot_product / (norm_a * norm_b)\n",
    "    \n",
    "    def make_user_recommendations_selected_cluster(self, cluster_items):\n",
    "        top_5_user_recitems = dict()\n",
    "        top_10_user_recitems = dict()\n",
    "        top_15_user_recitems = dict()\n",
    "        top_20_user_recitems = dict()\n",
    "        print(\"\\nMaking Recommendations\")\n",
    "        print(\"#Number of users randomly selected:\", len(self.userlist))\n",
    "        for user1 in self.userlist:                             # iterating through 10,000 randomly selected users\n",
    "            user1vec = self.user_vec.get(user1,[])              # getting the latent vector representation\n",
    "            if (user1vec != []):\n",
    "                self.user_simscore = {}\n",
    "                self.user_rec = {}\n",
    "                items1 = self.user_item.get(user1,[])           # getting the items purchased\n",
    "                for user2 in self.user_item:                    # iterating through other users in the gaming network\n",
    "                    user2vec = self.user_vec.get(user2,[])      # getting the latent vector representation\n",
    "                    if (user2vec != []):\n",
    "                        items2 = self.user_item[user2]          # getting the items purchased by these other users\n",
    "                        if (user1 == user2): continue\n",
    "                        rec_items = []\n",
    "                        cosine_sim_score = round(self.get_cosine_similarity(user1vec, user2vec), 3)  # getting user-user cosine similarity\n",
    "                        self.user_simscore[user2] = cosine_sim_score                                 # storing similarity values of each of iterated users with focal user\n",
    "                        for item in cluster_items:\n",
    "                            if item not in items1:\n",
    "                                if item in items2:\n",
    "                                    rec_items.append(item)\n",
    "                        self.user_rec[user2] = rec_items                                             # storing recommended items \n",
    "                self.sorted_user_simscore = sorted(self.user_simscore.items(), key=lambda x: x[1], reverse = True)\n",
    "                top_5_user_recitems[user1] = self.get_top_k_user_items(k=5)\n",
    "                top_10_user_recitems[user1] = self.get_top_k_user_items(k=10)\n",
    "                top_15_user_recitems[user1] = self.get_top_k_user_items(k=15)\n",
    "                top_20_user_recitems[user1] = self.get_top_k_user_items(k=20)\n",
    "        print(\"\\n\")\n",
    "        print(\"Accuracy for GCN Recommender System based on Top-5 similar users\")\n",
    "        self.get_accuracy_score(user_recitems = top_5_user_recitems)\n",
    "        print(\"\\n\")\n",
    "        print(\"Accuracy for GCN Recommender System based on Top-10 similar users\")\n",
    "        self.get_accuracy_score(user_recitems = top_10_user_recitems)\n",
    "        print(\"\\n\")\n",
    "        print(\"Accuracy for GCN Recommender System based on Top-15 similar users\")\n",
    "        self.get_accuracy_score(user_recitems = top_15_user_recitems)\n",
    "        print(\"\\n\")\n",
    "        print(\"Accuracy for GCN Recommender System based on Top-20 similar users\")\n",
    "        self.get_accuracy_score(user_recitems = top_20_user_recitems)\n",
    "        print(\"\\n\")\n",
    "        \n",
    "    def get_top_k_user_items(self, k):\n",
    "        top_k_users = [user for user,_ in self.sorted_user_simscore[0:k]]   # getting the top k most similar users to the focal user\n",
    "        top_k_users_items = set()                                           # now we fetch the recommended items for these top users\n",
    "        for user in top_k_users:\n",
    "            items = self.user_rec[user]\n",
    "            for item in items:\n",
    "                top_k_users_items.add(item)\n",
    "        return list(top_k_users_items)      \n",
    "        \n",
    "    def get_accuracy_score(self, user_recitems):\n",
    "        count_2days = count_4days = count_10days = 0\n",
    "        for user in user_recitems:\n",
    "            recommended_items = set(user_recitems.get(user,[]))\n",
    "            new_items_2days = set(self.test2days_user_item.get(user,[]))\n",
    "            new_items_4days = set(self.test4days_user_item.get(user,[]))\n",
    "            new_items_10days = set(self.test10days_user_item.get(user,[]))\n",
    "            if len(recommended_items & new_items_2days) > 0:\n",
    "                count_2days += 1\n",
    "            if len(recommended_items & new_items_4days) > 0:\n",
    "                count_4days += 1\n",
    "            if len(recommended_items & new_items_10days) > 0:\n",
    "                count_10days += 1\n",
    "        print(\"For a 2-day testing period:\", count_2days/10000)\n",
    "        print(\"For a 4-day testing period:\", count_4days/10000)\n",
    "        print(\"For a 10-day testing period:\", count_10days/10000)\n",
    "        \n",
    "    \n",
    "    def make_item_recommendations(self):\n",
    "        with open(dirpath + \"/rank_item_quantity_price_sales.txt\") as itemfile:  # pull the ranked items, and create clusters\n",
    "            for line in itemfile:\n",
    "                toks = line.strip().split(\"\\t\")\n",
    "                if len(toks) == 5:\n",
    "                    self.all_items.append(toks[1])\n",
    "                    rank = int(toks[0])\n",
    "                    if rank <= 40:\n",
    "                        self.star_items.append(toks[1])\n",
    "                    else:\n",
    "                        self.tail_items.append(toks[1])\n",
    "        #print(self.star_items)\n",
    "        #print(self.tail_items)\n",
    "        #print(self.all_items)\n",
    "        print(\"\\nMaking Recommendations\\n\")\n",
    "        print(\"\\nAccuracy for Star Items\\n\")\n",
    "        self.make_user_recommendations_selected_cluster(cluster_items = self.star_items)\n",
    "        print(\"\\nAccuracy for Long Tail Items\\n\")\n",
    "        self.make_user_recommendations_selected_cluster(cluster_items = self.tail_items)\n",
    "        print(\"\\nAccuracy for All Items\\n\")\n",
    "        self.make_user_recommendations_selected_cluster(cluster_items = self.all_items)\n",
    "                                            \n",
    "dirpath = 'C:/Vidit/PhD/RA Work/KZ - RA/Gaming Project/data/Files'\n",
    "outputfilename = 'C:/Vidit/PhD/RA Work/KZ - RA/Gaming Project/data/GCN/GCN_Gaming_Base_Network_Embeddings.txt'\n",
    "\n",
    "\n",
    "def main():\n",
    "    rvi = RecommendVirtualItems()\n",
    "    rvi.read_data(dirpath)\n",
    "    rvi.read_gcn_output(outputfilename)\n",
    "    rvi.make_item_recommendations()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
